{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark._\n",
       "import org.apache.spark.sql._\n",
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.ml.linalg.Vectors\n",
       "import org.apache.spark.ml.regression.LinearRegression\n",
       "import spark.implicits._\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.ml.linalg.Vectors\n",
    "import org.apache.spark.ml.regression.LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inputLines: org.apache.spark.rdd.RDD[String] = data/regression.txt MapPartitionsRDD[1] at textFile at <console>:35\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// SparkSession available as 'spark'\n",
    "val inputLines = spark.sparkContext.textFile(\"data/regression.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- X variables를 Vector로 묶어줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: org.apache.spark.rdd.RDD[(Double, org.apache.spark.ml.linalg.Vector)] = MapPartitionsRDD[7] at map at <console>:37\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = inputLines.map(_.split(\",\")).map(x => (x(0).toDouble, Vectors.dense(x(1).toDouble)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res9: Array[(Double, org.apache.spark.ml.linalg.Vector)] = Array((-1.74,[1.66]), (1.24,[-1.18]), (0.29,[-0.4]), (-0.13,[0.09]), (-0.39,[0.38]), (-1.79,[1.73]), (0.71,[-0.77]), (1.39,[-1.48]), (1.15,[-1.43]), (0.13,[-0.07]))\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\n",
       "colNames: Seq[String] = List(label, features)\n",
       "df: org.apache.spark.sql.DataFrame = [label: double, features: vector]\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "val colNames = Seq(\"label\", \"features\")\n",
    "val df = data.toDF(colNames: _*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "|label|features|\n",
      "+-----+--------+\n",
      "|-1.74|  [1.66]|\n",
      "| 1.24| [-1.18]|\n",
      "| 0.29|  [-0.4]|\n",
      "|-0.13|  [0.09]|\n",
      "|-0.39|  [0.38]|\n",
      "|-1.79|  [1.73]|\n",
      "| 0.71| [-0.77]|\n",
      "| 1.39| [-1.48]|\n",
      "| 1.15| [-1.43]|\n",
      "| 0.13| [-0.07]|\n",
      "| 0.05| [-0.07]|\n",
      "|  1.9|  [-1.8]|\n",
      "| 1.48| [-1.42]|\n",
      "| 0.32|  [-0.3]|\n",
      "|-1.11|   [1.0]|\n",
      "| 0.51| [-0.62]|\n",
      "|-1.58|  [1.45]|\n",
      "|-0.46|  [0.44]|\n",
      "|-0.49|  [0.37]|\n",
      "| 0.31|  [-0.3]|\n",
      "+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainTest: Array[org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]] = Array([label: double, features: vector], [label: double, features: vector])\n",
       "trainingDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: double, features: vector]\n",
       "testDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: double, features: vector]\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trainTest = df.randomSplit(Array(0.5, 0.5))\n",
    "val trainingDF = trainTest(0)\n",
    "val testDF = trainTest(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lir: org.apache.spark.ml.regression.LinearRegression = linReg_a635d25c73c9\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lir = new LinearRegression()\n",
    "  .setRegParam(0.3) // regularization \n",
    "  .setElasticNetParam(0.8) // elastic net mixing\n",
    "  .setMaxIter(100) // max iterations\n",
    "  .setTol(1E-6) // convergence tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-05 21:33:55 WARN  BLAS:61 - Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "2019-04-05 21:33:55 WARN  BLAS:61 - Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model: org.apache.spark.ml.regression.LinearRegressionModel = linReg_a635d25c73c9\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Train the model using our training data\n",
    "val model = lir.fit(trainingDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습된 모델로 test 데이터 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fullPredictions: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: double, features: vector ... 1 more field]\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fullPredictions = model.transform(testDF).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-------------------+\n",
      "|label|features|         prediction|\n",
      "+-----+--------+-------------------+\n",
      "|-3.74|  [3.75]| -2.697115171264099|\n",
      "|-2.36|  [2.63]| -1.892269537161224|\n",
      "|-2.09|  [1.97]|-1.4179855027791723|\n",
      "|-2.07|  [2.04]| -1.468288354910602|\n",
      "| -2.0|  [2.02]|-1.4539161114444794|\n",
      "|-1.94|  [1.98]|-1.4251716245122337|\n",
      "|-1.91|  [1.83]| -1.317379798516313|\n",
      "|-1.91|  [1.86]|-1.3389381637154971|\n",
      "|-1.87|  [1.98]|-1.4251716245122337|\n",
      "| -1.8|  [1.84]|-1.3245659202493745|\n",
      "|-1.75|  [1.69]|-1.2167740942534535|\n",
      "|-1.74|  [1.66]|-1.1952157290542693|\n",
      "|-1.66|  [1.64]|-1.1808434855881464|\n",
      "|-1.65|  [1.63]|-1.1736573638550851|\n",
      "|-1.64|  [1.84]|-1.3245659202493745|\n",
      "|-1.61|  [1.72]|-1.2383324594526377|\n",
      "|-1.53|  [1.68]|-1.2095879725203922|\n",
      "|-1.47|  [1.46]|-1.0514932943930415|\n",
      "|-1.42|  [1.59]|-1.1449128769228398|\n",
      "| -1.4|  [1.32]|-0.9508875901301823|\n",
      "+-----+--------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fullPredictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictionAndLabel: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[66] at map at <console>:70\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictionAndLabel = fullPredictions.select(\"prediction\", \"label\")\n",
    "    .rdd.map(x => (x.getDouble(0), x.getDouble(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-2.697115171264099,-3.74)\n",
      "(-1.892269537161224,-2.36)\n",
      "(-1.4179855027791723,-2.09)\n",
      "(-1.468288354910602,-2.07)\n",
      "(-1.4539161114444794,-2.0)\n",
      "(-1.4251716245122337,-1.94)\n",
      "(-1.317379798516313,-1.91)\n",
      "(-1.3389381637154971,-1.91)\n",
      "(-1.4251716245122337,-1.87)\n",
      "(-1.3245659202493745,-1.8)\n"
     ]
    }
   ],
   "source": [
    "// (y pred, y true)\n",
    "predictionAndLabel.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Stop the session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scala-spark",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
