{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# key/values RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{age: number of people}\n",
    "\n",
    "- functions\n",
    "    - reduceByKey()\n",
    "    - groupByKey()\n",
    "    - sortByKey()\n",
    "    - keys(), values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://163.152.185.244:4041\n",
       "SparkContext available as 'sc' (version = 2.3.1, master = local[*], app id = local-1552908203569)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark._\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lines: org.apache.spark.rdd.RDD[String] = ./data/fakefriends.csv MapPartitionsRDD[1] at textFile at <console>:28\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lines = sc.textFile(\"./data/fakefriends.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1 col: user id\n",
    "- 2 col: name\n",
    "- 3 col: user age\n",
    "- 4 col: #friends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,Will,33,385\n",
      "1,Jean-Luc,26,2\n",
      "2,Hugh,55,221\n",
      "3,Deanna,40,465\n",
      "4,Quark,68,21\n",
      "5,Weyoun,59,318\n",
      "6,Gowron,37,220\n",
      "7,Will,54,307\n",
      "8,Jadzia,38,380\n",
      "9,Hugh,27,181\n"
     ]
    }
   ],
   "source": [
    "lines.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fields: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[44] at map at <console>:30\n"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fields = lines.map(line => line.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[45] at map at <console>:32\n",
       "numFriends: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[46] at map at <console>:33\n"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val age = fields.map(field => field(2).toInt )\n",
    "val numFriends = fields.map(field => field(3).toInt )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Extract two arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "26\n",
      "55\n",
      "40\n",
      "68\n",
      "59\n",
      "37\n",
      "54\n",
      "38\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "age.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385\n",
      "2\n",
      "221\n",
      "465\n",
      "21\n",
      "318\n",
      "220\n",
      "307\n",
      "380\n",
      "181\n"
     ]
    }
   ],
   "source": [
    "numFriends.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- zip the two arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "merged: org.apache.spark.rdd.RDD[(Int, Int)] = ZippedPartitionsRDD2[48] at zip at <console>:35\n"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val merged = age.zip(numFriends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33,385)\n",
      "(26,2)\n",
      "(55,221)\n",
      "(40,465)\n",
      "(68,21)\n",
      "(59,318)\n",
      "(37,220)\n",
      "(54,307)\n",
      "(38,380)\n",
      "(27,181)\n"
     ]
    }
   ],
   "source": [
    "merged.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As a function\n",
    "    - apply each row to parse it out and the procssed output append\n",
    "    - for appeding, def = {element}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "parseLine: (line: String)(Int, Int)\n"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parseLine(line: String)={ \n",
    "    val fields = line.split(\",\")\n",
    "    val age = fields(2).toInt\n",
    "    val numFriends = fields(3).toInt\n",
    "    (age, numFriends) // will be appended as element\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[17] at map at <console>:38\n"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = lines.map(parseLine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33,385)\n",
      "(26,2)\n",
      "(55,221)\n",
      "(40,465)\n",
      "(68,21)\n",
      "(59,318)\n",
      "(37,220)\n",
      "(54,307)\n",
      "(38,380)\n",
      "(27,181)\n"
     ]
    }
   ],
   "source": [
    "rdd.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **mapValues**: value의 형태를 바꾸고 싶을 때 사용되는 함수\n",
    "    - transform value(x) into something(x,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "value_expand: org.apache.spark.rdd.RDD[(Int, (Int, Int))] = MapPartitionsRDD[41] at mapValues at <console>:40\n"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val value_expand = rdd.mapValues(x => (x,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33,(385,1))\n",
      "(26,(2,1))\n",
      "(55,(221,1))\n",
      "(40,(465,1))\n",
      "(68,(21,1))\n",
      "(59,(318,1))\n",
      "(37,(220,1))\n",
      "(54,(307,1))\n",
      "(38,(380,1))\n",
      "(27,(181,1))\n"
     ]
    }
   ],
   "source": [
    "value_expand.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **reduceByKey**: key에 대해서 각 연산을 적용하고 싶을 때 사용\n",
    "    - key 마다 각각 연산\n",
    "    - unique key마다 2개의 example들을 선택해 친구의 수와 해당 사람의 수를 누적해서 더해 나감\n",
    "        - input\n",
    "            - (33, (385, 1)) \n",
    "            - (33, (2,1))\n",
    "        - output\n",
    "            - (33, (387, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "key_cum: org.apache.spark.rdd.RDD[(Int, (Int, Int))] = ShuffledRDD[42] at reduceByKey at <console>:43\n"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//reduceByKey(현재 value, 누적 value) => (현재 value 1 + 누적 value 1, 현재 value 2 + 누적 value 2)\n",
    "val key_cum = value_expand.reduceByKey((x,y)=> (x._1 + y._1, x._2 + y._2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (unique key, (#friends, count key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34,(1473,6))\n",
      "(52,(3747,11))\n",
      "(56,(1840,6))\n",
      "(66,(2488,9))\n",
      "(22,(1445,7))\n",
      "(28,(2091,10))\n",
      "(54,(3615,13))\n",
      "(46,(2908,13))\n",
      "(48,(2814,10))\n",
      "(30,(2594,11))\n"
     ]
    }
   ],
   "source": [
    "key_cum.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- x = values = (#friends, count key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "averageByAge: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[43] at mapValues at <console>:44\n"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val averageByAge = key_cum.mapValues(x => x._1/x._2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34,245)\n",
      "(52,340)\n",
      "(56,306)\n",
      "(66,276)\n",
      "(22,206)\n",
      "(28,209)\n",
      "(54,278)\n",
      "(46,223)\n",
      "(48,281)\n",
      "(30,235)\n"
     ]
    }
   ],
   "source": [
    "averageByAge.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "results: Array[(Int, Int)] = Array((34,245), (52,340), (56,306), (66,276), (22,206), (28,209), (54,278), (46,223), (48,281), (30,235), (50,254), (32,207), (36,246), (24,233), (62,220), (64,281), (42,303), (40,250), (18,343), (20,165), (38,193), (58,116), (44,282), (60,202), (26,242), (68,269), (19,213), (39,169), (41,268), (61,256), (21,350), (47,233), (55,295), (53,222), (25,197), (29,215), (59,220), (65,298), (35,211), (27,228), (57,258), (51,302), (33,325), (37,249), (23,246), (45,309), (63,384), (67,214), (69,235), (49,184), (31,267), (43,230))\n"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val results = averageByAge.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- array slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res71: (Int, Int) = (34,245)\n"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res72: Array[(Int, Int)] = Array((34,245), (52,340))\n"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.slice(0,2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scala-spark",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
