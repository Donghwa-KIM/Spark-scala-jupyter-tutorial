{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD to sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark._\n",
       "import org.apache.spark.SparkContext._\n",
       "import org.apache.spark.sql._\n",
       "import org.apache.log4j._\n",
       "import scala.io.Codec\n",
       "import scala.io.Source\n",
       "import java.nio.charset.CodingErrorAction\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.spark.SparkContext._\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.log4j._\n",
    "import scala.io.Codec\n",
    "import scala.io.Source\n",
    "import java.nio.charset.CodingErrorAction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set the log level to only print errors\n",
    "Logger.getLogger(\"org\").setLevel(Level.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1 col: user id\n",
    "- 2 col: movie id\n",
    "- 3 col: rating\n",
    "- 4 col: timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lines: org.apache.spark.rdd.RDD[String] = data/u.data MapPartitionsRDD[1] at textFile at <console>:38\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lines = sc.textFile(\"data/u.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196\t242\t3\t881250949\n",
      "186\t302\t3\t891717742\n",
      "22\t377\t1\t878887116\n",
      "244\t51\t2\t880606923\n",
      "166\t346\t1\t886397596\n"
     ]
    }
   ],
   "source": [
    "lines.take(5).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mID: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[2] at map at <console>:40\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//RDD\n",
    "val mID = lines.map(_.split(\"\\t\")(1).toInt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242\n",
      "302\n",
      "377\n",
      "51\n",
      "346\n"
     ]
    }
   ],
   "source": [
    "mID.take(5).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "moviesDS: org.apache.spark.sql.DataFrame = [movieID: int]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Convert records of the RDD (mID) to Rows and DF\n",
    "val moviesDS = mID.map(x=> x.toInt).toDF(\"movieID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|movieID|\n",
      "+-------+\n",
      "|    242|\n",
      "|    302|\n",
      "|    377|\n",
      "|     51|\n",
      "|    346|\n",
      "|    474|\n",
      "|    265|\n",
      "|    465|\n",
      "|    451|\n",
      "|     86|\n",
      "|    257|\n",
      "|   1014|\n",
      "|    222|\n",
      "|     40|\n",
      "|     29|\n",
      "|    785|\n",
      "|    387|\n",
      "|    274|\n",
      "|   1042|\n",
      "|   1184|\n",
      "+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "moviesDS.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- group count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|movieID|count|\n",
      "+-------+-----+\n",
      "|    496|  231|\n",
      "|    471|  221|\n",
      "|    463|   71|\n",
      "|    148|  128|\n",
      "|   1342|    2|\n",
      "|    833|   49|\n",
      "|   1088|   13|\n",
      "|   1591|    6|\n",
      "|   1238|    8|\n",
      "|   1580|    1|\n",
      "|   1645|    1|\n",
      "|    392|   68|\n",
      "|    623|   39|\n",
      "|    540|   43|\n",
      "|    858|    3|\n",
      "|    737|   59|\n",
      "|    243|  132|\n",
      "|   1025|   44|\n",
      "|   1084|   21|\n",
      "|   1127|   11|\n",
      "+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "moviesDS\n",
    "    .groupBy(\"movieID\")\n",
    "    .count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- group count descending ordered by the count\n",
    "- cache(): 결과를 계속 재 생성하는 것이 아니라 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topMovieIDs: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [movieID: int, count: bigint]\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val topMovieIDs = moviesDS\n",
    "    .groupBy(\"movieID\")\n",
    "    .count()\n",
    "    .orderBy(desc(\"count\"))\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|movieID|count|\n",
      "+-------+-----+\n",
      "|     50|  583|\n",
      "|    258|  509|\n",
      "|    100|  508|\n",
      "|    181|  507|\n",
      "|    294|  485|\n",
      "|    286|  481|\n",
      "|    288|  478|\n",
      "|      1|  452|\n",
      "|    300|  431|\n",
      "|    121|  429|\n",
      "|    174|  420|\n",
      "|    127|  413|\n",
      "|     56|  394|\n",
      "|      7|  392|\n",
      "|     98|  390|\n",
      "|    237|  384|\n",
      "|    117|  378|\n",
      "|    172|  367|\n",
      "|    222|  365|\n",
      "|    204|  350|\n",
      "+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topMovieIDs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "top10: Array[org.apache.spark.sql.Row] = Array([50,583], [258,509], [100,508], [181,507], [294,485], [286,481], [288,478], [1,452], [300,431], [121,429])\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val top10 = topMovieIDs.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50,583]\n",
      "[258,509]\n",
      "[100,508]\n",
      "[181,507]\n",
      "[294,485]\n",
      "[286,481]\n",
      "[288,478]\n",
      "[1,452]\n",
      "[300,431]\n",
      "[121,429]\n"
     ]
    }
   ],
   "source": [
    "top10.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- u.item()\n",
    "    - 1 col: User ID\n",
    "    - 2 col: Movie ID\n",
    "    - 3 col: Rating\n",
    "    - 4 col: Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loadMovieNames: ()Map[Int,String]\n",
       "names: Map[Int,String] = Map(645 -> Paris Is Burning (1990), 892 -> Flubber (1997), 69 -> Forrest Gump (1994), 1322 -> Metisse (Caf� au Lait) (1993), 1665 -> Brother's Kiss, A (1997), 1036 -> Drop Dead Fred (1991), 1586 -> Lashou shentan (1992), 1501 -> Prisoner of the Mountains (Kavkazsky Plennik) (1996), 809 -> Rising Sun (1993), 1337 -> Larger Than Life (1996), 1411 -> Barbarella (1968), 629 -> Victor/Victoria (1982), 1024 -> Mrs. Dalloway (1997), 1469 -> Tom and Huck (1995), 365 -> Powder (1995), 1369 -> Forbidden Christ, The (Cristo proibito, Il) (1950), 138 -> D3: The Mighty Ducks (1996), 1190 -> That Old Feeling (1997), 1168 -> Little Buddha (1993), 760 -> Screamers (1995), 101 -> Heavy Metal (1981), 1454 -> Angel and the Badman (1947), 1633 -> �..."
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loadMovieNames() : Map[Int, String] = {\n",
    "\n",
    "// Handle character encoding issues:\n",
    "implicit val codec = Codec(\"UTF-8\")\n",
    "codec.onMalformedInput(CodingErrorAction.REPLACE)\n",
    "codec.onUnmappableCharacter(CodingErrorAction.REPLACE)\n",
    "\n",
    "// Create a Map of Intsgroup count to Strings, and populate it from u.item.\n",
    "var movieNames:Map[Int, String] = Map()\n",
    "\n",
    " val lines = Source.fromFile(\"data/u.item\").getLines()\n",
    " for (line <- lines) {\n",
    "   var fields = line.split('|')\n",
    "   if (fields.length > 1) {\n",
    "    // (User ID -> Moive ID)\n",
    "    movieNames += (fields(0).toInt -> fields(1))\n",
    "   }\n",
    " }\n",
    "\n",
    "\n",
    " return movieNames\n",
    "}\n",
    "val names = loadMovieNames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Star Wars (1977): 583\n",
      "Contact (1997): 509\n",
      "Fargo (1996): 508\n",
      "Return of the Jedi (1983): 507\n",
      "Liar Liar (1997): 485\n",
      "English Patient, The (1996): 481\n",
      "Scream (1996): 478\n",
      "Toy Story (1995): 452\n",
      "Air Force One (1997): 431\n",
      "Independence Day (ID4) (1996): 429\n"
     ]
    }
   ],
   "source": [
    "for (result <- top10) {\n",
    "  // result is just a Row at this point; we need to cast it back.\n",
    "  // Each row has movieID, count as above.\n",
    "  println (names(result(0).asInstanceOf[Int]) + \": \" + result(1))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Stop the session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scala-spark",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
