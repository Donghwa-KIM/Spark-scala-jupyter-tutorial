{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark._\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- one2one mapping\n",
    "- one example $\\rightarrow$ one result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lines: org.apache.spark.rdd.RDD[String] = ./data/redfox.txt MapPartitionsRDD[9] at textFile at <console>:34\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lines = sc.textFile(\"./data/redfox.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List(The quick red fox jumped over the lazy brown dogs, I am donghwa)\n"
     ]
    }
   ],
   "source": [
    "println(lines.collect().toList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 대문자로 변환해주는 RDD생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rageCaps: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[10] at map at <console>:36\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rageCaps = lines.map(x => x.toUpperCase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE QUICK RED FOX JUMPED OVER THE LAZY BROWN DOGS\n"
     ]
    }
   ],
   "source": [
    "println(rageCaps.collect().toList(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# flatMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- flatMap는 아웃풋이 single value가 아니라 list(the set of values)로 반환 해준다.\n",
    "    - map: Array(Array(a),Array(b),Array(c),Array(d))\n",
    "    - flatMap: Array((a,b,c,d))\n",
    "- 사용된 목적은 아웃풋의 shape이 flatten되다는 특징이 있다. 만약 문장마다 parsed 단어를 유지하고 싶으면 flatMap 대신 map을 사용하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_words: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[11] at map at <console>:36\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val _words = lines.map(x => x.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res18: Array[Array[String]] = Array(Array(The, quick, red, fox, jumped, over, the, lazy, brown, dogs), Array(I, am, donghwa))\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_words.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- flatmap을 사용하면 여러개의 value가 flatten된 array가 산출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "words: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[12] at flatMap at <console>:36\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val words = lines.flatMap(x => x.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res19: Array[String] = Array(The, quick, red, fox, jumped, over, the, lazy, brown, dogs, I, am, donghwa)\n"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "input: org.apache.spark.rdd.RDD[String] = ./data/book.txt MapPartitionsRDD[16] at textFile at <console>:34\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val input = sc.textFile(\"./data/book.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-Employment: Building an Internet Business of One\n",
      "Achieving Financial and Personal Freedom through a Lifestyle Technology Business\n",
      "By Frank Kane\n",
      "\n",
      "\n",
      "\n",
      "Copyright � 2015 Frank Kane. \n",
      "All rights reserved worldwide.\n"
     ]
    }
   ],
   "source": [
    "input.take(8).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "words: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[17] at flatMap at <console>:36\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val words = input.flatMap( x => x.split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 문장단위의 구분없이 map 아웃풋이 flatten되었다는 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-Employment:\n",
      "Building\n",
      "an\n",
      "Internet\n",
      "Business\n",
      "of\n",
      "One\n",
      "Achieving\n",
      "Financial\n",
      "and\n",
      "Personal\n",
      "Freedom\n",
      "through\n",
      "a\n",
      "Lifestyle\n"
     ]
    }
   ],
   "source": [
    "words.take(15).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordCounts: scala.collection.Map[String,Long] = Map(foolproof -> 1, precious -> 1, inflammatory -> 1, referrer, -> 1, hourly -> 3, embedded -> 1, way). -> 1, touch, -> 1, of. -> 3, salesperson -> 5, Leeches -> 1, expansion. -> 1, rate -> 7, appropriate. -> 2, CPA�s -> 1, 2014 -> 2, WELL-MEANING -> 1, Talk -> 5, Much -> 1, Builder\" -> 1, plugin -> 3, headache -> 1, purchasing -> 9, China\" -> 1, looks -> 2, site, -> 7, ranking -> 2, scare -> 1, hard-earned -> 1, freedom� -> 1, Seattle, -> 3, PULLING -> 1, action. -> 1, accident -> 3, scale. -> 2, looking. -> 1, physically -> 1, 27, -> 1, call. -> 1, contracts -> 3, twofold. -> 1, scenario -> 1, Advertising -> 3, way? -> 2, nudge -> 1, gamble -> 1, ideas -> 19, sketches -> 1, static -> 1, freelancer. -> 1, �PR:� -> 1, joining -> 1, particu..."
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wordCounts= words.countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(foolproof,1)\n",
      "(precious,1)\n",
      "(inflammatory,1)\n",
      "(referrer,,1)\n",
      "(hourly,3)\n",
      "(embedded,1)\n",
      "(way).,1)\n",
      "(touch,,1)\n",
      "(of.,3)\n",
      "(salesperson,5)\n"
     ]
    }
   ],
   "source": [
    "wordCounts.take(10).foreach(println)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scala-spark",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
