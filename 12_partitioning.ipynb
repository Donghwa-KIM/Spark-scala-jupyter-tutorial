{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partitioning\n",
    "    - large operation을 효율적으로 사용하기 위해서 데이터를 partitioning을 하여 분산처리\n",
    "        - large operation: join(), reduceByKey(), groupBykey()\n",
    "    - executor를 더 많이 가져가는 방법\n",
    "    - 너무 많이가져가도 비효율적\n",
    "        - 너무 많이 쪼개고, 붙이면 오히려 비 효율적\n",
    "    - spark 작동 원리\n",
    "        1) shuffle data\n",
    "        2) individual tasks로 구분\n",
    "        3) distributed to each node(executor) of the cluster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- groupBy()전에 moviePairs를 partitioning함\n",
    "\n",
    "```scala\n",
    "val moviePairs = uniqueJoinedRatings.map(makePairs).partitionBy(new HashPartitioner(100))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13 예제에서 partitionning을 적용한 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://163.152.---.---:----\n",
       "SparkContext available as 'sc' (version = 2.3.1, master = local[*], app id = local-1554289479030)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark._\n",
       "import scala.io.{Codec, Source}\n",
       "import java.nio.charset.CodingErrorAction\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark._\n",
    "import scala.io.{Codec, Source}\n",
    "import java.nio.charset.CodingErrorAction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "codec: scala.io.Codec = UTF-8\n",
       "res0: codec.type = UTF-8\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "implicit val codec = Codec(\"UTF-8\")\n",
    "codec.onMalformedInput(CodingErrorAction.REPLACE)\n",
    "codec.onUnmappableCharacter(CodingErrorAction.REPLACE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lines: Iterator[String] = non-empty iterator\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lines = Source.fromFile(\"data/u.item\").getLines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tmp: String = 1|Toy Story (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Toy%20Story%20(1995)|0|0|0|1|1|1|0|0|0|0|0|0|0|0|0|0|0|0|0\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tmp = lines.toList.take(1)(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: Array[String] = Array(1, Toy Story (1995), 01-Jan-1995, \"\", http://us.imdb.com/M/title-exact?Toy%20Story%20(1995), 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.split(\"\\\\|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loadMovieNames: ()Map[Int,String]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loadMovieNames() : Map[Int, String] = {    \n",
    "    \n",
    "    var movieNames: Map[Int, String] = Map()\n",
    "    val lines = Source.fromFile(\"data/u.item\").getLines()\n",
    "\n",
    "    for (line <- lines) {\n",
    "      val fields = line.split(\"\\\\|\")\n",
    "      movieNames += (fields(0).toInt -> fields(1))\n",
    "    }\n",
    "\n",
    "    movieNames\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "movieNames: Map[Int,String] = Map(645 -> Paris Is Burning (1990), 892 -> Flubber (1997), 69 -> Forrest Gump (1994), 1322 -> Metisse (Caf� au Lait) (1993), 1665 -> Brother's Kiss, A (1997), 1036 -> Drop Dead Fred (1991), 1586 -> Lashou shentan (1992), 1501 -> Prisoner of the Mountains (Kavkazsky Plennik) (1996), 809 -> Rising Sun (1993), 1337 -> Larger Than Life (1996), 1411 -> Barbarella (1968), 629 -> Victor/Victoria (1982), 1024 -> Mrs. Dalloway (1997), 1469 -> Tom and Huck (1995), 365 -> Powder (1995), 1369 -> Forbidden Christ, The (Cristo proibito, Il) (1950), 138 -> D3: The Mighty Ducks (1996), 1190 -> That Old Feeling (1997), 1168 -> Little Buddha (1993), 760 -> Screamers (1995), 101 -> Heavy Metal (1981), 1454 -> Angel and the Badman (1947), 1633 -> � k�ldum klaka (Cold Fever) (1..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val movieNames = loadMovieNames()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parseLine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: org.apache.spark.rdd.RDD[String] = data/u.data MapPartitionsRDD[1] at textFile at <console>:32\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = sc.textFile(\"data/u.data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1 col: User ID\n",
    "- 2 col: Movie ID\n",
    "- 3 col: Rating\n",
    "- 4 col: Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: Array[String] = Array(196, 242, 3, 881250949)\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.take(1)(0).split(\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined type alias Rating\n",
       "parseLine: (line: String)(Int, Rating)\n",
       "ratings: org.apache.spark.rdd.RDD[(Int, Rating)] = MapPartitionsRDD[2] at map at <console>:41\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type Rating = (Int, Double)\n",
    "\n",
    "def parseLine(line: String): (Int, Rating) = {\n",
    "    val fields = line.split(\"\\t\")\n",
    "    (fields(0).toInt, (fields(1).toInt, fields(2).toDouble))\n",
    "}\n",
    "\n",
    "val ratings = data.map(parseLine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: org.apache.spark.rdd.RDD[(Int, Rating)] = MapPartitionsRDD[2] at map at <console>:41\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(196,(242,3.0))\n",
      "(186,(302,3.0))\n",
      "(22,(377,1.0))\n",
      "(244,(51,2.0))\n",
      "(166,(346,1.0))\n",
      "(298,(474,4.0))\n",
      "(115,(265,2.0))\n",
      "(253,(465,5.0))\n",
      "(305,(451,3.0))\n",
      "(6,(86,3.0))\n"
     ]
    }
   ],
   "source": [
    "ratings.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "shipMap: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[3] at parallelize at <console>:32\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val shipMap = sc.parallelize(Array((1, \"Enterprise\"),\n",
    "                  (1, \"Enterprise-D\"),\n",
    "                  (2, \"Deep Space Nine\"),\n",
    "                  (2 -> \"Voyager\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 자기 자신을 join할 경우 같은 key(user)에 대한 모든 조합의 경우의 수(두개의 영화)로 concat됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "joined_shipMap: org.apache.spark.rdd.RDD[(Int, (String, String))] = MapPartitionsRDD[6] at join at <console>:34\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val joined_shipMap = shipMap.join(shipMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,(Enterprise,Enterprise))\n",
      "(2,(Deep Space Nine,Deep Space Nine))\n",
      "(1,(Enterprise,Enterprise-D))\n",
      "(2,(Deep Space Nine,Voyager))\n",
      "(1,(Enterprise-D,Enterprise))\n",
      "(2,(Voyager,Deep Space Nine))\n",
      "(1,(Enterprise-D,Enterprise-D))\n",
      "(2,(Voyager,Voyager))\n"
     ]
    }
   ],
   "source": [
    "joined_shipMap.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "joinedRatings: org.apache.spark.rdd.RDD[(Int, (Rating, Rating))] = MapPartitionsRDD[9] at join at <console>:38\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val joinedRatings = ratings.join(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(778,((94,2.0),(94,2.0)))\n",
      "(778,((94,2.0),(78,1.0)))\n",
      "(778,((94,2.0),(7,4.0)))\n",
      "(778,((94,2.0),(1273,3.0)))\n",
      "(778,((94,2.0),(265,4.0)))\n"
     ]
    }
   ],
   "source": [
    "joinedRatings.take(5).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# unique filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ratingPair: (Int, (Rating, Rating)) = (778,((94,2.0),(78,1.0)))\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ratingPair = joinedRatings.take(2)(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "firstRating: Rating = (94,2.0)\n",
       "secondRating: Rating = (78,1.0)\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val firstRating = ratingPair._2._1\n",
    "val secondRating = ratingPair._2._2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res7: Int = 94\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstRating._1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res8: Double = 1.0\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "secondRating._2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res9: Boolean = false\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstRating._1 < secondRating._2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filterDuplicates: (ratingPair: (Int, (Rating, Rating)))Boolean\n",
       "uniqueJoinedRatings: org.apache.spark.rdd.RDD[(Int, (Rating, Rating))] = MapPartitionsRDD[10] at filter at <console>:53\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " def filterDuplicates(ratingPair: (Int, (Rating, Rating))): \n",
    "    Boolean = {\n",
    "    val firstRating = ratingPair._2._1\n",
    "    val secondRating = ratingPair._2._2\n",
    "    //unique\n",
    "    firstRating._1 < secondRating._1\n",
    "  }\n",
    "\n",
    "val uniqueJoinedRatings = joinedRatings.filter(filterDuplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(778,((94,2.0),(1273,3.0)))\n",
      "(778,((94,2.0),(265,4.0)))\n",
      "(778,((94,2.0),(239,4.0)))\n",
      "(778,((94,2.0),(193,4.0)))\n",
      "(778,((94,2.0),(1035,1.0)))\n",
      "(778,((94,2.0),(616,4.0)))\n",
      "(778,((94,2.0),(230,2.0)))\n",
      "(778,((94,2.0),(582,1.0)))\n",
      "(778,((94,2.0),(262,4.0)))\n",
      "(778,((94,2.0),(238,3.0)))\n"
     ]
    }
   ],
   "source": [
    "uniqueJoinedRatings.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# makePairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_ratings: (Int, (Rating, Rating)) = (778,((94,2.0),(1273,3.0)))\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val _ratings = uniqueJoinedRatings.take(1)(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_firstRating: Rating = (94,2.0)\n",
       "_secondRating: Rating = (1273,3.0)\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val _firstRating = _ratings._2._1\n",
    "val _secondRating = _ratings._2._2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res11: ((Int, Int), (Double, Double)) = ((94,1273),(2.0,3.0))\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(_firstRating._1, _secondRating._1) -> (_firstRating._2, _secondRating._2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "makePairs: (ratings: (Int, (Rating, Rating)))((Int, Int), (Double, Double))\n",
       "moviePairs: org.apache.spark.rdd.RDD[((Int, Int), (Double, Double))] = ShuffledRDD[12] at partitionBy at <console>:55\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def makePairs(ratings: (Int, (Rating, Rating))): ((Int, Int), (Double, Double)) = {\n",
    "    val firstRating = ratings._2._1\n",
    "    val secondRating = ratings._2._2\n",
    "\n",
    "    (firstRating._1, secondRating._1) -> (firstRating._2, secondRating._2)\n",
    "  }\n",
    "\n",
    "val moviePairs = uniqueJoinedRatings.map(makePairs).partitionBy(new HashPartitioner(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((7,405),(4.0,3.0))\n",
      "((239,496),(4.0,1.0))\n",
      "((193,262),(4.0,4.0))\n",
      "((262,268),(4.0,2.0))\n",
      "((262,712),(4.0,3.0))\n",
      "((121,209),(3.0,4.0))\n",
      "((204,738),(4.0,1.0))\n",
      "((234,629),(3.0,2.0))\n",
      "((180,197),(4.0,4.0))\n",
      "((11,367),(5.0,5.0))\n"
     ]
    }
   ],
   "source": [
    "// ((movie_id1, movie_id2),(rating1, rating2))\n",
    "moviePairs.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "moviePairsRatings: org.apache.spark.rdd.RDD[((Int, Int), Iterable[(Double, Double)])] = MapPartitionsRDD[13] at groupByKey at <console>:51\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val moviePairsRatings = moviePairs.groupByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  mapvalues\n",
    "    - value에 대해서만 연산하여 (key, value)출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "shipMap: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[14] at parallelize at <console>:32\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val shipMap = sc.parallelize(Array((1, \"Enterprise\"),\n",
    "                  (1, \"Enterprise-D\"),\n",
    "                  (2, \"Deep Space Nine\"),\n",
    "                  (2 -> \"Voyager\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- x $\\rightarrow$ tuple(1, \"Enterprise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res13: Array[Int] = Array(1, 1, 2, 2)\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shipMap.map(x => x._1).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For each key, x $\\rightarrow$ value(\"Enterprise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res14: Array[(Int, Char)] = Array((1,E), (1,E), (2,D), (2,V))\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shipMap.mapValues(x => x(0)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "things: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[17] at parallelize at <console>:32\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val things =sc.parallelize(List((\"animal\", \"bear\"),\n",
    "             (\"animal\", \"duck\"),\n",
    "             (\"plant\", \"cactus\"), \n",
    "             (\"vehicle\", \"speed boat\"),\n",
    "             (\"vehicle\", \"school bus\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(plant,CompactBuffer(cactus))\n",
      "(animal,CompactBuffer(bear, duck))\n",
      "(vehicle,CompactBuffer(speed boat, school bus))\n"
     ]
    }
   ],
   "source": [
    "things.groupByKey().foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numPairs: Int = 0\n",
       "sum_xx: Double = 0.0\n",
       "sum_yy: Double = 0.0\n",
       "sum_xy: Double = 0.0\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var numPairs: Int = 0\n",
    "var sum_xx: Double = 0.0\n",
    "var sum_yy: Double = 0.0\n",
    "var sum_xy: Double = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res16: org.apache.spark.rdd.RDD[((Int, Int), Iterable[(Double, Double)])] = MapPartitionsRDD[13] at groupByKey at <console>:51\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// iterator = (key, value)\n",
    "// moviePairsRatings = (key, iterator)\n",
    "moviePairsRatings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res17: org.apache.spark.rdd.RDD[((Int, Int), (Double, Double))] = ShuffledRDD[12] at partitionBy at <console>:55\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moviePairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tmp: (Int, Int) = (94,1273)\n",
       "ratingPairs: Array[(Double, Double)] = Array((2.0,3.0), (5.0,2.0), (2.0,2.0), (4.0,3.0), (2.0,2.0), (3.0,2.0), (2.0,2.0), (4.0,2.0), (4.0,3.0))\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tmp : (Int, Int) = (94,1273)\n",
    "val ratingPairs = moviePairs.filter(x =>  x._1 == tmp).map(x=>x._2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2.0,3.0)\n",
      "(5.0,2.0)\n",
      "(2.0,2.0)\n",
      "(4.0,3.0)\n",
      "(2.0,2.0)\n",
      "(3.0,2.0)\n",
      "(2.0,2.0)\n",
      "(4.0,2.0)\n",
      "(4.0,3.0)\n"
     ]
    }
   ],
   "source": [
    "ratingPairs.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pair: (Double, Double) = (2.0,3.0)\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pair = ratingPairs(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ratingX: Double = 2.0\n",
       "ratingY: Double = 3.0\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ratingX = pair._1\n",
    "val ratingY = pair._2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_xx += ratingX * ratingX\n",
    "sum_yy += ratingY * ratingY\n",
    "sum_xy += ratingX * ratingY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "numPairs += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res21: (Double, Double) = (2.0,3.0)\n"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratingPairs(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numPairs: Int = 9\n",
       "sum_xx: Double = 98.0\n",
       "sum_yy: Double = 51.0\n",
       "sum_xy: Double = 66.0\n"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var numPairs: Int = 0\n",
    "var sum_xx: Double = 0.0\n",
    "var sum_yy: Double = 0.0\n",
    "var sum_xy: Double = 0.0\n",
    "\n",
    "for (pair <- ratingPairs) {\n",
    "  val ratingX = pair._1\n",
    "  val ratingY = pair._2\n",
    "\n",
    "  sum_xx += ratingX * ratingX\n",
    "  sum_yy += ratingY * ratingY\n",
    "  sum_xy += ratingX * ratingY\n",
    "  numPairs += 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numerator: Double = 66.0\n",
       "denominator: Double = 70.69653456853455\n"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numerator: Double = sum_xy\n",
    "val denominator = Math.sqrt(sum_xx) * Math.sqrt(sum_yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "score: Double = 0.9335676833780072\n"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var score:Double = 0.0\n",
    "if (denominator != 0) {\n",
    "  score = numerator / denominator\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res24: (Double, Int) = (0.9335676833780072,9)\n"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(score, numPairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- cache(): reuse the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "computeCosineSimilarity: (ratingPairs: Iterable[(Double, Double)])(Double, Int)\n",
       "moviePairSimilarities: org.apache.spark.rdd.RDD[((Int, Int), (Double, Int))] = MapPartitionsRDD[21] at mapValues at <console>:97\n"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def computeCosineSimilarity(ratingPairs: Iterable[(Double, Double)]): (Double, Int) = {\n",
    "    var numPairs: Int = 0\n",
    "    var sum_xx: Double = 0.0\n",
    "    var sum_yy: Double = 0.0\n",
    "    var sum_xy: Double = 0.0\n",
    "\n",
    "    for (pair <- ratingPairs) {\n",
    "      val ratingX = pair._1\n",
    "      val ratingY = pair._2\n",
    "\n",
    "      sum_xx += ratingX * ratingX\n",
    "      sum_yy += ratingY * ratingY\n",
    "      sum_xy += ratingX * ratingY\n",
    "      numPairs += 1\n",
    "    }\n",
    "\n",
    "    val numerator: Double = sum_xy\n",
    "    val denominator = Math.sqrt(sum_xx) * Math.sqrt(sum_yy)\n",
    "\n",
    "    var score:Double = 0.0\n",
    "    if (denominator != 0) {\n",
    "      score = numerator / denominator\n",
    "    }\n",
    "\n",
    "    (score, numPairs)\n",
    "}\n",
    "val moviePairSimilarities = moviePairsRatings.mapValues(computeCosineSimilarity).cache() //reuse results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scoreThreshold: Double = 0.6\n",
       "coOccurenceThreshold: Double = 50.0\n",
       "movieId: Int = 94\n",
       "filteredResults: org.apache.spark.rdd.RDD[((Int, Int), (Double, Int))] = MapPartitionsRDD[22] at filter at <console>:77\n",
       "results: Array[((Double, Int), (Int, Int))] = Array(((0.9586049379734621,66),(73,94)), ((0.9540093745210545,124),(94,204)), ((0.953888417198362,117),(94,174)), ((0.9514502380030145,67),(94,742)), ((0.9513610928045688,80),(94,208)), ((0.9512871292224748,105),(79,94)), ((0.9511736470315294,56),(94,232)), ((0.9511281290554398,65),(94,692)), ((0.9487881988287913,66),(94,258)), ((0.9481276017281098,119),(94,210)))\n"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val scoreThreshold = 0.60\n",
    "val coOccurenceThreshold = 50.0\n",
    "var movieId = 94 \n",
    "val filteredResults = moviePairSimilarities.filter(x => {\n",
    "    //pair = (movie_id1, movie_id2)\n",
    "    val pair = x._1\n",
    "    //sim = (rating_sim, num)\n",
    "    val sim = x._2\n",
    "    (pair._1 == movieId || pair._2 == movieId) &&\n",
    "      sim._1 > scoreThreshold && sim._2 > coOccurenceThreshold\n",
    "})\n",
    "val results = filteredResults.map(x => (x._2, x._1)).sortByKey(ascending = false).take(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- movieName = Map(idx => name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top similar movies (max of 10) for Home Alone (1990)\n"
     ]
    }
   ],
   "source": [
    "println(\"\\nTop similar movies (max of 10) for \" + movieNames(movieId))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res26: Array[((Double, Int), (Int, Int))] = Array(((0.9586049379734621,66),(73,94)), ((0.9540093745210545,124),(94,204)), ((0.953888417198362,117),(94,174)), ((0.9514502380030145,67),(94,742)), ((0.9513610928045688,80),(94,208)), ((0.9512871292224748,105),(79,94)), ((0.9511736470315294,56),(94,232)), ((0.9511281290554398,65),(94,692)), ((0.9487881988287913,66),(94,258)), ((0.9481276017281098,119),(94,210)))\n"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//(sim, (movie_id1, movie_id2))\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "result: ((Double, Int), (Int, Int)) = ((0.9586049379734621,66),(73,94))\n"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val result = results(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_sim: (Double, Int) = (0.9586049379734621,66)\n",
       "_pair: (Int, Int) = (73,94)\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val _sim = result._1\n",
    "val _pair = result._2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "similarMovieId: Int = 73\n"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var similarMovieId = _pair._1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res27: Int = 94\n"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movieId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res28: Boolean = false\n"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarMovieId == movieId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (result <- results) {\n",
    "    val sim = result._1\n",
    "    val pair = result._2\n",
    "\n",
    "    var similarMovieId = pair._1\n",
    "    if (similarMovieId == movieId) {\n",
    "      similarMovieId = pair._2\n",
    "    }\n",
    "    println(movieNames(similarMovieId) + \"\\tscore: \" + sim._1 + \"\\tstrength: \" + sim._2)\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scala-spark",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
