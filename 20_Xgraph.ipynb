{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vertex RDD: 노드, degree of connectedness\n",
    "- EdgeRDD: 두개의 노드가 연결된 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://163.152.---.---:----\n",
       "SparkContext available as 'sc' (version = 2.3.1, master = local[*], app id = local-1554673019494)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark._\n",
       "import org.apache.spark.SparkContext._\n",
       "import org.apache.spark.sql._\n",
       "import org.apache.log4j._\n",
       "import org.apache.spark.rdd.RDD\n",
       "import org.apache.spark.graphx._\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.spark.SparkContext._\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.log4j._\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.graphx._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- make vertex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "names: org.apache.spark.rdd.RDD[String] = ./data/Marvel-names.txt MapPartitionsRDD[1] at textFile at <console>:41\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val names = sc.textFile(\"./data/Marvel-names.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- for example. first row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fields: Array[String] = Array(\"1 \", 24-HOUR MAN/EMMANUEL)\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var fields = names.take(1)(0).split(\"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "heroID: Long = 1\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val heroID:Long = fields(0).trim().toLong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: Some[(Long, String)] = Some((1,24-HOUR MAN/EMMANUEL))\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Some( fields(0).trim().toLong, fields(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "parseNames: (line: String)Option[(org.apache.spark.graphx.VertexId, String)]\n",
       "verts: org.apache.spark.rdd.RDD[(org.apache.spark.graphx.VertexId, String)] = MapPartitionsRDD[2] at flatMap at <console>:59\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parseNames(line: String) : Option[(VertexId, String)] = {\n",
    "    var fields = line.split(\"\\\"\")\n",
    "    if (fields.length > 1) {\n",
    "      val heroID:Long = fields(0).trim().toLong\n",
    "      if (heroID < 6487) {  // ID's above 6486 aren't real characters\n",
    "        return Some( fields(0).trim().toLong, fields(1))\n",
    "      }\n",
    "    } \n",
    "\n",
    "    return None // flatmap will just discard None results, and extract data from Some results.\n",
    "}\n",
    "\n",
    "val verts = names.flatMap(parseNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 그래프의 vertex 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,24-HOUR MAN/EMMANUEL)\n",
      "(2,3-D MAN/CHARLES CHAN)\n",
      "(3,4-D MAN/MERCURIO)\n",
      "(4,8-BALL/)\n",
      "(5,A)\n",
      "(6,A'YIN)\n",
      "(7,ABBOTT, JACK)\n",
      "(8,ABCISSA)\n",
      "(9,ABEL)\n",
      "(10,ABOMINATION/EMIL BLO)\n"
     ]
    }
   ],
   "source": [
    "verts.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- make edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lines: org.apache.spark.rdd.RDD[String] = ./data/Marvel-graph.txt MapPartitionsRDD[4] at textFile at <console>:41\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lines = sc.textFile(\"./data/Marvel-graph.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- for example. first row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import scala.collection.mutable.ListBuffer\n",
       "_edges: scala.collection.mutable.ListBuffer[org.apache.spark.graphx.Edge[Int]] = ListBuffer()\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.collection.mutable.ListBuffer\n",
    "var _edges = new ListBuffer[Edge[Int]]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_fields: Array[String] = Array(5988, 748, 1722, 3752, 4655, 5743, 1872, 3413, 5527, 6368, 6085, 4319, 4728, 1636, 2397, 3364, 4001, 1614, 1819, 1585, 732, 2660, 3952, 2507, 3891, 2070, 2239, 2602, 612, 1352, 5447, 4548, 1596, 5488, 1605, 5517, 11, 479, 2554, 2043, 17, 865, 4292, 6312, 473, 534, 1479, 6375, 4456)\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val _fields = lines.take(1)(0).split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_origin: String = 5988\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val _origin = _fields(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Edge: xgraph function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (x <- 1 to (_fields.length - 1)) {\n",
    "  // Our attribute field is unused, but in other graphs could\n",
    "  // be used to deep track of physical distances etc.\n",
    "  _edges += Edge(_origin.toLong, _fields(x).toLong, 0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: scala.collection.mutable.ListBuffer[org.apache.spark.graphx.Edge[Int]] = ListBuffer(Edge(5988,748,0), Edge(5988,1722,0), Edge(5988,3752,0), Edge(5988,4655,0), Edge(5988,5743,0), Edge(5988,1872,0), Edge(5988,3413,0), Edge(5988,5527,0), Edge(5988,6368,0), Edge(5988,6085,0), Edge(5988,4319,0), Edge(5988,4728,0), Edge(5988,1636,0), Edge(5988,2397,0), Edge(5988,3364,0), Edge(5988,4001,0), Edge(5988,1614,0), Edge(5988,1819,0), Edge(5988,1585,0), Edge(5988,732,0), Edge(5988,2660,0), Edge(5988,3952,0), Edge(5988,2507,0), Edge(5988,3891,0), Edge(5988,2070,0), Edge(5988,2239,0), Edge(5988,2602,0), Edge(5988,612,0), Edge(5988,1352,0), Edge(5988,5447,0), Edge(5988,4548,0), Edge(5988,1596,0), Edge(5988,5488,0), Edge(5988,1605,0), Edge(5988,5517,0), Edge(5988,11,0), Edge(5988,479,0), Edge(5988,..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모든 데이터 쌍에 대해서 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "makeEdges: (line: String)List[org.apache.spark.graphx.Edge[Int]]\n",
       "edges: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[Int]] = MapPartitionsRDD[5] at flatMap at <console>:62\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def makeEdges(line: String) : List[Edge[Int]] = {\n",
    "    import scala.collection.mutable.ListBuffer\n",
    "    var edges = new ListBuffer[Edge[Int]]()\n",
    "    val fields = line.split(\" \")\n",
    "    val origin = fields(0)\n",
    "    for (x <- 1 to (fields.length - 1)) {\n",
    "      // Our attribute field is unused, but in other graphs could\n",
    "      // be used to deep track of physical distances etc.\n",
    "      edges += Edge(origin.toLong, fields(x).toLong, 0)\n",
    "    }\n",
    "\n",
    "    return edges.toList\n",
    "}\n",
    "// \n",
    "val edges = lines.flatMap(makeEdges)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 그래프의 edge 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge(5988,748,0)\n",
      "Edge(5988,1722,0)\n",
      "Edge(5988,3752,0)\n",
      "Edge(5988,4655,0)\n",
      "Edge(5988,5743,0)\n",
      "Edge(5988,1872,0)\n",
      "Edge(5988,3413,0)\n",
      "Edge(5988,5527,0)\n",
      "Edge(5988,6368,0)\n",
      "Edge(5988,6085,0)\n"
     ]
    }
   ],
   "source": [
    "edges.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- make the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "default: String = Nobody\n",
       "graph: org.apache.spark.graphx.Graph[String,Int] = org.apache.spark.graphx.impl.GraphImpl@714eaeb0\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val default = \"Nobody\"\n",
    "val graph = Graph(verts, edges, default).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 각 노드에 대한 degree 산출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4904,136)\n",
      "(1084,526)\n",
      "(384,42)\n",
      "(6400,30)\n",
      "(3702,34)\n",
      "(6308,178)\n",
      "(5618,38)\n",
      "(5354,176)\n",
      "(1894,16)\n",
      "(4926,22)\n"
     ]
    }
   ],
   "source": [
    "//((node,degree),...)\n",
    "graph.degrees.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 두 RDD에 대해서 key 맵핑\n",
    "    - graph.degrees = (key, degree)\n",
    "    - verts = (key, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4904,(136,SCATTERBRAIN))\n",
      "(1084,(526,CLOAK/TYRONE JOHNSON))\n",
      "(3586,(88,MEACHUM, WARD))\n",
      "(6400,(30,ZA'KEN))\n",
      "(3702,(34,MIST))\n",
      "(6308,(178,WOLVERINE DOPPELGANG))\n",
      "(5618,(38,TALO))\n",
      "(5354,(176,STACY, GWEN))\n",
      "(1894,(16,FIRESTAR DOPPELGANGE))\n",
      "(4926,(22,SCOTT, JAKE))\n"
     ]
    }
   ],
   "source": [
    "graph.degrees.join(verts).take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sortBy(_._2._1,ascending=false)\n",
    "    - _ = (4904,(136,SCATTERBRAIN))\n",
    "    - _.2 = (136,SCATTERBRAIN)\n",
    "    - _.2.1 = 136\n",
    "    - decreasing sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.degrees.join(verts).take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 most-connected superheroes:\n",
      "(859,(3866,CAPTAIN AMERICA))\n",
      "(5306,(3482,SPIDER-MAN/PETER PAR))\n",
      "(2664,(3056,IRON MAN/TONY STARK ))\n",
      "(5716,(2852,THING/BENJAMIN J. GR))\n",
      "(6306,(2788,WOLVERINE/LOGAN ))\n",
      "(3805,(2772,MR. FANTASTIC/REED R))\n",
      "(2557,(2742,HUMAN TORCH/JOHNNY S))\n",
      "(4898,(2690,SCARLET WITCH/WANDA ))\n",
      "(5736,(2578,THOR/DR. DONALD BLAK))\n",
      "(403,(2560,BEAST/HENRY &HANK& P))\n"
     ]
    }
   ],
   "source": [
    "println(\"\\nTop 10 most-connected superheroes:\")\n",
    "// The join merges the hero names into the output; sorts by total connections on each node.\n",
    "graph.degrees.join(verts).sortBy(_._2._1, ascending=false).take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing degrees of separation from SpiderMan...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "root: org.apache.spark.graphx.VertexId = 5306\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Now let's do Breadth-First Search using the Pregel API\n",
    "println(\"\\nComputing degrees of separation from SpiderMan...\")\n",
    "\n",
    "// Start from SpiderMan\n",
    "val root: VertexId = 5306 // SpiderMan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BFS 알고리즘 방식의 그래프 초기화 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "initialGraph: org.apache.spark.graphx.Graph[Double,Int] = org.apache.spark.graphx.impl.GraphImpl@6935e720\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Initialize each node with a distance of infinity, unless it's our starting point\n",
    "val initialGraph = graph.mapVertices((id, _) => if (id == root) 0.0 else Double.PositiveInfinity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 10 iteration\n",
    "- shortest path(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-08 11:19:34 WARN  BlockManager:66 - Block rdd_235_1 already exists on this machine; not re-adding it\n",
      "2019-04-08 11:19:34 WARN  BlockManager:66 - Block rdd_235_0 already exists on this machine; not re-adding it\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "bfs: org.apache.spark.graphx.Graph[Double,Int] = org.apache.spark.graphx.impl.GraphImpl@3db87249\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// graph update\n",
    "val bfs = initialGraph.pregel(Double.PositiveInfinity, 10)( \n",
    "    // 가장 짧은 거리 구하기\n",
    "    //(id, attr, msg) = (node, attrubute, message)\n",
    "    (id, attr, msg) => math.min(attr, msg), \n",
    "    // 각 노드의 이웃 노드들을 업데이트 후 1 추가\n",
    "    triplet => { \n",
    "      if (triplet.srcAttr != Double.PositiveInfinity) { \n",
    "        Iterator((triplet.dstId, triplet.srcAttr+1)) \n",
    "      } else { \n",
    "        Iterator.empty \n",
    "      } \n",
    "    }, \n",
    "\n",
    "    // 누적 거리 구하기\n",
    "    (a,b) => math.min(a,b) ).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4904,1.0)\n",
      "(1084,1.0)\n",
      "(384,2.0)\n",
      "(6400,2.0)\n",
      "(3702,2.0)\n",
      "(6308,1.0)\n",
      "(5618,2.0)\n",
      "(5354,1.0)\n",
      "(1894,2.0)\n",
      "(4926,2.0)\n"
     ]
    }
   ],
   "source": [
    "bfs.vertices.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- start node(spider-man)와 다른 노드의 거리\n",
    "    - 4904(SCATTERBRAIN)는 spider-man과 거리가 1 떨어져 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4904,(1.0,SCATTERBRAIN))\n",
      "(1084,(1.0,CLOAK/TYRONE JOHNSON))\n",
      "(3586,(2.0,MEACHUM, WARD))\n",
      "(6400,(2.0,ZA'KEN))\n",
      "(3702,(2.0,MIST))\n",
      "(6308,(1.0,WOLVERINE DOPPELGANG))\n",
      "(5618,(2.0,TALO))\n",
      "(5354,(1.0,STACY, GWEN))\n",
      "(1894,(2.0,FIRESTAR DOPPELGANGE))\n",
      "(4926,(2.0,SCOTT, JAKE))\n"
     ]
    }
   ],
   "source": [
    "// Print out the first 100 results:\n",
    "bfs.vertices.join(verts).take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Degrees from SpiderMan to ADAM 3,031\n",
      "(14,2.0)\n"
     ]
    }
   ],
   "source": [
    "// Recreate our \"degrees of separation\" result:\n",
    "println(\"\\n\\nDegrees from SpiderMan to ADAM 3,031\")  // ADAM 3031 is hero ID 14\n",
    "// x = (id, distance)\n",
    "bfs.vertices.filter(x => x._1 == 14).collect.foreach(println)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scala-spark",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
