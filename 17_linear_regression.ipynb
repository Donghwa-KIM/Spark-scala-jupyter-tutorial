{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://163.152.185.244:4041\n",
       "SparkContext available as 'sc' (version = 2.3.1, master = local[*], app id = local-1554465514496)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark._\n",
       "import org.apache.spark.sql._\n",
       "import org.apache.spark.mllib.linalg.Vectors\n",
       "import org.apache.spark.mllib.regression.LabeledPoint\n",
       "import org.apache.spark.mllib.regression.LinearRegressionWithSGD\n",
       "import org.apache.spark.mllib.optimization.SquaredL2Updater\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "import org.apache.spark.mllib.regression.LabeledPoint\n",
    "import org.apache.spark.mllib.regression.LinearRegressionWithSGD\n",
    "import org.apache.spark.mllib.optimization.SquaredL2Updater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainingLines: org.apache.spark.rdd.RDD[String] = data/regression.txt MapPartitionsRDD[1] at textFile at <console>:35\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trainingLines = sc.textFile(\"data/regression.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.74,1.66\n",
      "1.24,-1.18\n",
      "0.29,-0.40\n",
      "-0.13,0.09\n",
      "-0.39,0.38\n",
      "-1.79,1.73\n",
      "0.71,-0.77\n",
      "1.39,-1.48\n",
      "1.15,-1.43\n",
      "0.13,-0.07\n"
     ]
    }
   ],
   "source": [
    "trainingLines.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "testingLines: org.apache.spark.rdd.RDD[String] = data/regression-test.txt MapPartitionsRDD[24] at textFile at <console>:36\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// same as training\n",
    "val testingLines = sc.textFile(\"data/regression-test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 변수와 타켓 구분\n",
    "    - (y,[X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainingData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[25] at map at <console>:39\n",
       "testData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[26] at map at <console>:40\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trainingData = trainingLines.map(LabeledPoint.parse).cache()\n",
    "val testData = testingLines.map(LabeledPoint.parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-1.74,[1.66])\n",
      "(1.24,[-1.18])\n",
      "(0.29,[-0.4])\n",
      "(-0.13,[0.09])\n",
      "(-0.39,[0.38])\n",
      "(-1.79,[1.73])\n",
      "(0.71,[-0.77])\n",
      "(1.39,[-1.48])\n",
      "(1.15,[-1.43])\n",
      "(0.13,[-0.07])\n"
     ]
    }
   ],
   "source": [
    "testData.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.66]\n",
      "[-1.18]\n",
      "[-0.4]\n",
      "[0.09]\n",
      "[0.38]\n",
      "[1.73]\n",
      "[-0.77]\n",
      "[-1.48]\n",
      "[-1.43]\n",
      "[-0.07]\n"
     ]
    }
   ],
   "source": [
    "// x variable\n",
    "testData.map( x => x.features).take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.74\n",
      "1.24\n",
      "0.29\n",
      "-0.13\n",
      "-0.39\n",
      "-1.79\n",
      "0.71\n",
      "1.39\n",
      "1.15\n",
      "0.13\n"
     ]
    }
   ],
   "source": [
    "// y target\n",
    "testData.map( x => x.label).take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "algorithm: org.apache.spark.mllib.regression.LinearRegressionWithSGD = org.apache.spark.mllib.regression.LinearRegressionWithSGD@1e7da171\n",
       "res9: algorithm.optimizer.type = org.apache.spark.mllib.optimization.GradientDescent@481acdc2\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val algorithm = new LinearRegressionWithSGD()\n",
    "\n",
    "algorithm.optimizer\n",
    "  .setNumIterations(100)\n",
    "  .setStepSize(1.0) //learning rate\n",
    "  .setUpdater(new SquaredL2Updater())\n",
    "  .setRegParam(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model: org.apache.spark.mllib.regression.LinearRegressionModel = org.apache.spark.mllib.regression.LinearRegressionModel: intercept = 0.0, numFeatures = 1\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model = algorithm.run(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictions: org.apache.spark.rdd.RDD[Double] = MapPartitionsRDD[38] at mapPartitions at GeneralizedLinearAlgorithm.scala:70\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictions = model.predict(testData.map(x => x.features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.636276212468243\n",
      "1.1631361028388716\n",
      "0.3942834246911429\n",
      "-0.08871377055550715\n",
      "-0.37456925345658576\n",
      "-1.705275811789193\n",
      "0.7589955925304501\n",
      "1.4588486713572286\n",
      "1.4095632432708358\n",
      "0.06899959932095001\n"
     ]
    }
   ],
   "source": [
    "predictions.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictionAndLabel: org.apache.spark.rdd.RDD[(Double, Double)] = ZippedPartitionsRDD2[43] at zip at <console>:48\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictionAndLabel = predictions.zip(testData.map(_.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-1.636276212468243,-1.74)\n",
      "(1.1631361028388716,1.24)\n",
      "(0.3942834246911429,0.29)\n",
      "(-0.08871377055550715,-0.13)\n",
      "(-0.37456925345658576,-0.39)\n",
      "(-1.705275811789193,-1.79)\n",
      "(0.7589955925304501,0.71)\n",
      "(1.4588486713572286,1.39)\n",
      "(1.4095632432708358,1.15)\n",
      "(0.06899959932095001,0.13)\n"
     ]
    }
   ],
   "source": [
    "predictionAndLabel.take(10).foreach(println)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scala-spark",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
