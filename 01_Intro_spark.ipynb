{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spark "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- scalable\n",
    "    - controller: driver program-spark context\n",
    "        - Write scripts\n",
    "    - Top: cluster manager(spark, YARN)\n",
    "        - clutsers for distributed learning\n",
    "\n",
    "- fast\n",
    "    - mapreduce보다 10배 빠름\n",
    "        - Dag engine: directed acyclic graph\n",
    "        \n",
    "        \n",
    "- Compatiable code\n",
    "    - scala, python, java\n",
    "    - scala를 사용하는 이유\n",
    "        - Spark는 scala로 쓰여져 있음\n",
    "        - scala funtion들이 분산처리하기 좋게 되어 있음\n",
    "        - simple code\n",
    "        - 비교적 python은 느림 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- scala는 python과 유사한 문법을 가짐\n",
    "    - python\n",
    "\n",
    "    ```python\n",
    "    nums = sc.parallelize([1,2,3,4])\n",
    "    squared = nums.map(lambda x: x*x).collect()\n",
    "    ```\n",
    "\n",
    "    - scala\n",
    "    \n",
    "    ```scala\n",
    "    val nums = sc.parallelize(List(1,2,3,4))\n",
    "    val squared = nums.map(x => x*x).collect()\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Components of Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SPARK CORE\n",
    "    - Spark Streaming\n",
    "        - R(Resilient:회복력)D(Distributed)D(Dataset) 처리/변환/수집\n",
    "        - 실시간으로 작은 배치 데이터를 처리\n",
    "    - Spark SQL\n",
    "        - 대용량 데이터에 query를 사용할 때 사용\n",
    "    - MLLib\n",
    "        - 대용량 데이터를 분산처리하여 머신러닝 학습\n",
    "    - GraphX\n",
    "        - 그래프 네트워크\n",
    "        - social network\n",
    "        - pregel(프레글)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RDD의 정의\n",
    "    - Resilient    \n",
    "    - Distributed\n",
    "    - Dataset\n",
    "- immutable dataset = fault-tolerant dataset\n",
    "- 데이터셋을 RDD로 캡슐화(Input $\\rightarrow$ RDD1)\n",
    "- 각 RDD lineage of the **deterministic** operation (RDD1 $\\rightarrow$ RDD2 $\\rightarrow$ ...))\n",
    "- RDD partition이 손실이 일어나더라도 fault-tolerant dataset에서 다시 계산할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전반적인 running proess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- spark driver or script\n",
    "    - sc:context object\n",
    "        - cluster를 실행하기 위해 sc생성 $\\rightarrow$ RDD\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- parallize function\n",
    "   - create RDD\n",
    "   - calling the spark context as sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nums: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:25\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val nums = sc.parallelize(List(1,2,3,4)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 굉장히 큰 txt파일도 분산처리 가능\n",
    "    ```scala\n",
    "    sc.textfile(\"c:/users/donghwa/gobs-o-text.txt\")\n",
    "    ```\n",
    "- s3n:// amazon 호환\n",
    "    ```scala\n",
    "    sc.textfile(\"s3n:/users/donghwa/gobs-o-text.txt\")\n",
    "    ```\n",
    "- hdfs:// hadoop 호환\n",
    "    ```scala\n",
    "    sc.textfile(\"hdfs:/users/donghwa/gobs-o-text.txt\")\n",
    "    ```\n",
    "    \n",
    "- hive sql에서도 데이터로 추출하여 RDD형식으로 바꿀 수 있음\n",
    "    ```scala\n",
    "    rows = hiveCtx.sql(\"SELECT name, age FROM users\")\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming RDD's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- map: apply a function\n",
    "    - RDD에 함수가 적용되면 새로운 RDD생성\n",
    "- flatmap \n",
    "    - map과 유사하지만 one verus one으로 적용되지 않고, one verus multiple로 적용\n",
    "    - 하나의 input(row)에 대해서 여러 output을 만들고 싶을때 사용\n",
    "- filter\n",
    "    - select according to boolen idx\n",
    "- distinct\n",
    "    - as a unique set\n",
    "- sample\n",
    "    - 무작위 샘플\n",
    "- cartesian product\n",
    "    - 두개의 RDD를 연산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- map example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at parallelize at <console>:25\n",
       "squares: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[2] at map at <console>:26\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(List(1,2,3,4)) \n",
    "val squares = rdd.map(x=> x*x) //1,4,9,16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "4\n",
      "9\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "// print rows\n",
    "squares.collect().foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- function example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "4\n",
      "9\n",
      "16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "squareIt: (x: Int)Int\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def squareIt(x: Int): \n",
    "    Int = {return x*x}\n",
    "\n",
    "rdd.map(squareIt).collect().foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- collect\n",
    "    - RDD의 모든 데이터를 수집하여 spark drivers에 전달\n",
    "- count\n",
    "    - RDD의 Row 갯수 산출\n",
    "- countByValue\n",
    "    - RDD의 Row의 unique 수 산출\n",
    "- take/top\n",
    "    - RDD의 head(10 rows)추출\n",
    "- reduce\n",
    "    - 특정 key에 대해서 하고자 하는 operation으로 RDD combine할 때 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- lazy evaluation\n",
    "    - DAG: RDD action을 불러오기 전에 그래프 생성할 때 지연되는 현상"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.makeshare.org/bbs/board.php?bo_table=Parts&wr_id=24"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scala-spark",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
